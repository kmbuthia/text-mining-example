{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Import needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Scrapping text\n",
    "\n",
    "First we extract all the RELEVANT words from the books. We will also remove stop words so that our files contained partially cleaned data. Note that with the way the words are collected from the OCR output, they are already tokenized because I am storing one word at a time and the OCR output has already split the words if I target the span tag within the body where the text is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def words_from_book(book_path, file_name):\n",
    "    book_path = \"gap-html/gap_DqQNAAAAYAAJ/\"\n",
    "    book_words = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    #Lets get all the html files for the first book\n",
    "    book_pages = listdir(book_path);\n",
    "    #We filter the links to make sure that we don't add links to directories in the folder with our html\n",
    "    book_pages = [f for f in os.listdir(book_path) if os.path.isfile(os.path.join(book_path, f))]\n",
    "    book_pages.sort()\n",
    "    #print(book_pages)\n",
    "    for i in book_pages:\n",
    "        soup = BeautifulSoup(open(book_path+i))\n",
    "\n",
    "        #print(soup.prettify())\n",
    "\n",
    "        words = soup.find_all('span','ocr_cinfo');\n",
    "        #words = soup.body.get_text()\n",
    "        #text_file = open(file_name, \"w\")\n",
    "        #text_file.write(words)\n",
    "        #text_file.close()\n",
    "\n",
    "        for x in words:\n",
    "            text = x.contents[0].strip(\".(),\\'\\\";:*[]<>\").replace(\" \", \"\")\n",
    "            if(len(text)>0):\n",
    "                #print (text)\n",
    "                if x not in stop_words:\n",
    "                    book_words.append(text)\n",
    "    \n",
    "    if(len(book_words)>0):\n",
    "        with open(file_name, 'w') as csvfile:\n",
    "            writer = csv.writer(csvfile, dialect='excel')\n",
    "            for i in book_words:\n",
    "                writer.writerow([i])\n",
    "    else:\n",
    "        print(\"No words found\")\n",
    "\n",
    "words_from_book(\"gap-html/gap_-C0BAAAAQAAJ/\", \"gap-scrapped/all_words_book_01.csv\")\n",
    "#folders = [f for f in os.listdir(\"gap-html\")]\n",
    "#counter = 0\n",
    "#for paths in folders:\n",
    "#    counter = counter+1\n",
    "#    words_from_book(\"gap-html/\"+paths+\"/\", \"gap-scrapped/all_words_book_0\"+counter+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is some code to practice reading and writing to CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Short code practice on how to write and read csv files\n",
    "fieldnames = ['first_name', 'last_name', 'all_names']\n",
    "with open('testing.csv', 'w') as csvfile:\n",
    "    #If you want to give header names use method commented below\n",
    "    #writer = csv.DictWriter(csvfile, fieldnames='')\n",
    "    writer = csv.writer(csvfile, dialect='excel')\n",
    "    for i in fieldnames:\n",
    "        writer.writerow([i])\n",
    "\n",
    "with open('testing.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=' ')\n",
    "    for row in reader:\n",
    "        print(', '.join(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Stemming and Tokenizing\n",
    "\n",
    "Now we open the file and stem and tokenize contents then add result to tokenized and stemmed array. We tokenize to separate any punctuation from words and we stem to obtain word roots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "all_words = []\n",
    "total_stemmed = []\n",
    "total_tokens = []\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "#Extracting text from csv files\n",
    "def create_content_list(file_path):\n",
    "    file_list = [f for f in os.listdir(file_path) if os.path.isfile(os.path.join(file_path, f))]\n",
    "    file_list.sort()\n",
    "    #print(file_list)\n",
    "    for file in file_list:\n",
    "        book_words = \"\"\n",
    "        with open(file_path+\"/\"+file, newline='') as csvfile:\n",
    "            reader = csv.reader(csvfile, delimiter=' ')\n",
    "            for row in reader:\n",
    "                text = str(row).strip(\"[]'\")\n",
    "                #Let's remove words with less than two characters\n",
    "                if(len(text)>2 and re.search('[a-zA-Z]', text)):\n",
    "                    book_words += text+\" \"\n",
    "                    #book_words.append(text)\n",
    "                    \n",
    "        all_words.append(book_words)\n",
    "    \n",
    "    #Now let's create another list of all the words but which have been stemmed\n",
    "    countTokens = len(all_words)\n",
    "    print(\"Count token list: \"+str(countTokens))\n",
    "    #create_tokens_and_stems(all_words)\n",
    "    #countStemmed = len(total_stemmed)\n",
    "    #print(\"Count stem list: \"+str(countStemmed))\n",
    "    \n",
    "    #For testing\n",
    "    #print(\"Count filtered tokens: \"+str(len(total_tokens)))\n",
    "    #print(\"Count all tokens: \"+str(len(all_words_tokenized)))\n",
    "    #print(total_tokens[:2000])\n",
    "    #print(\"\\n\\n\\n\")\n",
    "    #print(total_stemmed[:2000])\n",
    "    \n",
    "#Stemming function\n",
    "def create_tokens_and_stems(word_list):\n",
    "    for w in word_list:\n",
    "        #print(str(w))\n",
    "        tokens = [word for sent in nltk.sent_tokenize(w) for word in nltk.word_tokenize(sent)]\n",
    "        for token in tokens:\n",
    "            total_tokens.append(token)\n",
    "        #print(\"Count filtered tokens: \"+str(len(total_tokens)))\n",
    "        \n",
    "    print(\"Final token count: \"+str(len(total_tokens)))\n",
    "    writeToFile(\"gap-st/tokenized.csv\", total_tokens)\n",
    "    for t in total_tokens:\n",
    "        total_stemmed.append(stemmer.stem(t))\n",
    "    writeToFile(\"gap-st/tokenized_and_stemmed.csv\", total_stemmed)\n",
    "    return total_stemmed\n",
    "\n",
    "def create_tokens_only(word_list):\n",
    "    for w in word_list:\n",
    "        #print(str(w))\n",
    "        tokens = [word for sent in nltk.sent_tokenize(w) for word in nltk.word_tokenize(sent)]\n",
    "        for token in tokens:\n",
    "            total_tokens.append(token)\n",
    "        print(\"Count filtered tokens: \"+str(len(total_tokens)))\n",
    "        \n",
    "    print(\"Final token count: \"+str(len(total_tokens)))\n",
    "    writeToFile(\"gap-st/tokenized.csv\", total_tokens)\n",
    "    return total_tokens\n",
    "\n",
    "def writeToFile(file_name, data):\n",
    "    with open(file_name, 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile, dialect='excel')\n",
    "        for i in data:\n",
    "            writer.writerow([i])\n",
    "\n",
    "create_content_list('gap-scrapped')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we create data frame to store tokenized and stemmed words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Retrieve token and stem lists\n",
    "total_tokens = []\n",
    "total_stemmed = []\n",
    "with open('gap-st/tokenized.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=' ')\n",
    "    for row in reader:\n",
    "        text = str(row).strip(\"[]'\")\n",
    "        total_tokens.append(text)\n",
    "        #print(', '.join(row))\n",
    "        \n",
    "with open('gap-st/tokenized_and_stemmed.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=' ')\n",
    "    for row in reader:\n",
    "        text = str(row).strip(\"[]'\")\n",
    "        total_stemmed.append(text)\n",
    "        \n",
    "print(\"Tokens: \"+str(len(total_tokens)))\n",
    "print(\"Stemmed: \"+str(len(total_stemmed)))\n",
    "\n",
    "#Now we create a panda data frame to store the tokens and stems\n",
    "word_frame = pd.DataFrame({'words': total_tokens}, index = total_stemmed)\n",
    "print ('there are ' + str(word_frame.shape[0]) + ' items in word_frame')\n",
    "print (word_frame.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Document similarity (TF-IDF)\n",
    "(Term frequency-inverse document frequency)\n",
    "\n",
    "Now we create a panda data frame and use it to compute document similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Now we determine document similarity\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=0.2, max_df=0.8, max_features=300000, use_idf=True, ngram_range=(1,3))\n",
    "\n",
    "#%time tfidf_matrix = tfidf_vectorizer.fit_transform(total_stemmed) #fit the vectorizer to all_words\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(all_words) #fit the vectorizer to all_words\n",
    "\n",
    "print(tfidf_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "#print(terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# K-Means Clustering\n",
    "\n",
    "Now that we have the tf-idf matrix, we can carry out various clustering techniques. Let's start with k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "num_clusters = 5\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "%time km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()\n",
    "\n",
    "joblib.dump(km,  'gap_cluster.pkl')\n",
    "\n",
    "print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Now we create dictionary of book titles and word clusters\n",
    "\n",
    "#Uncomment this if clusters haven't been loaded\n",
    "km = joblib.load('gap_cluster.pkl')\n",
    "clusters = km.labels_.tolist()\n",
    "\n",
    "#First save titles to array\n",
    "titles = []\n",
    "with open('gap-titles/titles.csv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        text = str(row).strip(\"[]'\")\n",
    "        titles.append(text)\n",
    "        \n",
    "#print(all_words[0][:200])\n",
    "        \n",
    "books = { 'titles': titles, 'descriptions': all_words, 'cluster': clusters }\n",
    "\n",
    "print(\"Clusters: \"+str(len(clusters)))\n",
    "print(\"Titles: \"+str(len(titles)))\n",
    "print(\"Words: \"+str(len(all_words)))\n",
    "\n",
    "frame = pd.DataFrame(books, index = [clusters] , columns = ['titles', 'cluster'])\n",
    "\n",
    "frame['cluster'].value_counts()\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :100]: #replace 6 with n words per cluster\n",
    "        print(' %s' % word_frame.ix[terms[ind].split(',')].values.tolist()[0][0], end=',')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "    print(\"Cluster %d titles:\" % i, end='')\n",
    "    for title in frame.ix[i]['titles'].values.tolist():\n",
    "        print(' %s,' % title, end='')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Multi-dimensional scaling\n",
    "\n",
    "Now we create a 2D array of the data using the distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.manifold import MDS\n",
    "import mpld3\n",
    "\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)\n",
    "\n",
    "MDS()\n",
    "\n",
    "# convert two components as we're plotting points in a two-dimensional plane\n",
    "# \"precomputed\" because we provide a distance matrix\n",
    "# we will also specify `random_state` so the plot is reproducible.\n",
    "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
    "\n",
    "pos = mds.fit_transform(dist)  # shape (n_components, n_samples)\n",
    "\n",
    "xs, ys = pos[:, 0], pos[:, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Visualizing the data\n",
    "\n",
    "We use matplotlib and mpld3 (matpot library for d3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#set up colors per clusters using a dict\n",
    "cluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3', 3: '#e7298a', 4: '#66a61e'}\n",
    "\n",
    "#set up cluster names using a dict\n",
    "cluster_names = {0: 'Gothic, church, Julian, Strab, Persian, Christianity', \n",
    "                 1: 'Herod, Josephus, Jerusalem, judea, David, Jewish', \n",
    "                 2: 'Athenians, Minerva, Christ, Homer, Bacchus, stadia', \n",
    "                 3: 'Livius, Fabius, Publius, Volsci, Quintus, Carthaginian', \n",
    "                 4: 'nero, Otho, Vitellius, Galba, Tacitus, Suet'}\n",
    "\n",
    "#some ipython magic to show the matplotlib plots inline\n",
    "%matplotlib inline \n",
    "\n",
    "#create data frame that has the result of the MDS plus the cluster numbers and titles\n",
    "df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, title=titles))\n",
    "\n",
    "#group by cluster\n",
    "groups = df.groupby('label')\n",
    "\n",
    "\n",
    "# set up plot\n",
    "fig, ax = plt.subplots(figsize=(17, 9)) # set size\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "\n",
    "#iterate through groups to layer the plot\n",
    "#note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\n",
    "for name, group in groups:\n",
    "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, \n",
    "            label=cluster_names[name], color=cluster_colors[name], \n",
    "            mec='none')\n",
    "    ax.set_aspect('auto')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'x',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        bottom='off',      # ticks along the bottom edge are off\n",
    "        top='off',         # ticks along the top edge are off\n",
    "        labelbottom='off')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'y',         # changes apply to the y-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        left='off',      # ticks along the bottom edge are off\n",
    "        top='off',         # ticks along the top edge are off\n",
    "        labelleft='off')\n",
    "    \n",
    "#ax.legend(numpoints=1)  #show legend with only 1 point\n",
    "\n",
    "#add label in x,y position with the label as the film title\n",
    "for i in range(len(df)):\n",
    "    ax.text(df.ix[i]['x'], df.ix[i]['y'], df.ix[i]['title'], size=8)  \n",
    "\n",
    "    \n",
    "    \n",
    "plt.show() #show the plot\n",
    "\n",
    "#uncomment the below to save the plot if need be\n",
    "#plt.savefig('gap_clusters_small_noaxes.pdf', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now let's make it INTERACTIVE!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#define custom toolbar location\n",
    "class TopToolbar(mpld3.plugins.PluginBase):\n",
    "    \"\"\"Plugin for moving toolbar to top of figure\"\"\"\n",
    "\n",
    "    JAVASCRIPT = \"\"\"\n",
    "    mpld3.register_plugin(\"toptoolbar\", TopToolbar);\n",
    "    TopToolbar.prototype = Object.create(mpld3.Plugin.prototype);\n",
    "    TopToolbar.prototype.constructor = TopToolbar;\n",
    "    function TopToolbar(fig, props){\n",
    "        mpld3.Plugin.call(this, fig, props);\n",
    "    };\n",
    "\n",
    "    TopToolbar.prototype.draw = function(){\n",
    "      // the toolbar svg doesn't exist\n",
    "      // yet, so first draw it\n",
    "      this.fig.toolbar.draw();\n",
    "\n",
    "      // then change the y position to be\n",
    "      // at the top of the figure\n",
    "      this.fig.toolbar.toolbar.attr(\"x\", 150);\n",
    "      this.fig.toolbar.toolbar.attr(\"y\", 400);\n",
    "\n",
    "      // then remove the draw function,\n",
    "      // so that it is not called again\n",
    "      this.fig.toolbar.draw = function() {}\n",
    "    }\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.dict_ = {\"type\": \"toptoolbar\"}\n",
    "        \n",
    "#create data frame that has the result of the MDS plus the cluster numbers and titles\n",
    "df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, title=titles)) \n",
    "\n",
    "#group by cluster\n",
    "groups = df.groupby('label')\n",
    "\n",
    "#define custom css to format the font and to remove the axis labeling\n",
    "css = \"\"\"\n",
    "text.mpld3-text, div.mpld3-tooltip {\n",
    "  font-family:Arial, Helvetica, sans-serif;\n",
    "}\n",
    "\n",
    "g.mpld3-xaxis, g.mpld3-yaxis {\n",
    "display: none; }\n",
    "\n",
    "svg.mpld3-figure {\n",
    "margin-left: -100px;}\n",
    "\"\"\"\n",
    "\n",
    "# Plot \n",
    "fig, ax = plt.subplots(figsize=(14,6)) #set plot size\n",
    "ax.margins(0.03) # Optional, just adds 5% padding to the autoscaling\n",
    "\n",
    "#iterate through groups to layer the plot\n",
    "#note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\n",
    "for name, group in groups:\n",
    "    points = ax.plot(group.x, group.y, marker='o', linestyle='', ms=18, \n",
    "                     label=cluster_names[name], mec='none', \n",
    "                     color=cluster_colors[name])\n",
    "    ax.set_aspect('auto')\n",
    "    labels = [i for i in group.title]\n",
    "    \n",
    "    #set tooltip using points, labels and the already defined 'css'\n",
    "    tooltip = mpld3.plugins.PointHTMLTooltip(points[0], labels,\n",
    "                                       voffset=10, hoffset=10, css=css)\n",
    "    #connect tooltip to fig\n",
    "    mpld3.plugins.connect(fig, tooltip, TopToolbar())    \n",
    "    \n",
    "    #set tick marks as blank\n",
    "    ax.axes.get_xaxis().set_ticks([])\n",
    "    ax.axes.get_yaxis().set_ticks([])\n",
    "    \n",
    "    #set axis as blank\n",
    "    ax.axes.get_xaxis().set_visible(False)\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "    \n",
    "#ax.legend(numpoints=1) #show legend with only one dot\n",
    "\n",
    "mpld3.display() #show the plot\n",
    "\n",
    "#uncomment the below to export to html\n",
    "#html = mpld3.fig_to_html(fig)\n",
    "#print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Hierarchical clustering\n",
    "\n",
    "Here we use the ward clustering algorithm because it allows us to carry out heirarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "\n",
    "linkage_matrix = ward(dist) #define the linkage_matrix using ward clustering pre-computed distances\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 20)) # set size\n",
    "ax = dendrogram(linkage_matrix, orientation=\"right\", labels=titles);\n",
    "\n",
    "plt.tick_params(\\\n",
    "    axis= 'x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom='off',      # ticks along the bottom edge are off\n",
    "    top='off',         # ticks along the top edge are off\n",
    "    labelbottom='off')\n",
    "\n",
    "plt.tight_layout() #show plot with tight layout\n",
    "\n",
    "#uncomment below to save figure\n",
    "plt.savefig('gap_dendogram_clusters.png', dpi=200) #save figure as ward_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Latent Dirichlet Allocation\n",
    "\n",
    "As an added bonus, let us try and group the books by topic. We do this by using LDA to determine the top key words in each book and form topic groups based on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#strip any proper nouns (NNP) or plural proper nouns (NNPS) from a text\n",
    "from nltk.tag import pos_tag\n",
    "from gensim import corpora, models, similarities \n",
    "\n",
    "def strip_proppers_POS(text):\n",
    "    tagged = pos_tag(text.split()) #use NLTK's part of speech tagger\n",
    "    non_propernouns = [word for word,pos in tagged if pos != 'NNP' and pos != 'NNPS']\n",
    "    return non_propernouns\n",
    "\n",
    "#remove proper names\n",
    "%time preprocess = [strip_proppers_POS(doc) for doc in all_words]\n",
    "\n",
    "#tokenize\n",
    "#%time tokenized_text = [create_tokens_and_stems(text) for text in preprocess]\n",
    "\n",
    "#remove stop words\n",
    "#%time texts = [[word for word in text if word not in stopwords] for text in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#create a Gensim dictionary from the texts\n",
    "dictionary = corpora.Dictionary(preprocess)\n",
    "\n",
    "#remove extremes (similar to the min/max df step used when creating the tf-idf matrix)\n",
    "dictionary.filter_extremes(no_below=0.1, no_above=0.8)\n",
    "\n",
    "#convert the dictionary to a bag of words corpus for reference\n",
    "corpus = [dictionary.doc2bow(text) for text in preprocess]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%time lda = models.LdaModel(corpus, num_topics=5, id2word=dictionary, update_every=5, chunksize=500, passes=25)\n",
    "\n",
    "lda.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now let's show top 10 words in each created topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "topics_matrix = lda.show_topics(formatted=False, num_words=10)\n",
    "#topics_matrix = np.array(topics_matrix)\n",
    "\n",
    "#topic_words = topics_matrix[:,:,1]\n",
    "for i in topics_matrix:\n",
    "    print([str(word) for word in i])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
